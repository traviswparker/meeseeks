#!/usr/bin/env python3

import sys
import logging
import signal
import os
import time
import threading
import glob

from meeseeks.util import cmdline_parser, read_cfg_files
from meeseeks.client import Client
from meeseeks.watch import Watch

global CFG,WATCH,CLIENT,SHUTDOWN
CLIENT=None
CFG={'cleanup':300} #loaded config
WATCH={} #watch threads we're running 
SHUTDOWN=threading.Event()   

def usage():

    print("""
    meeseeks-watch [key=value]... [config-file]

    watches files, submits jobs on them

    JSON config format, can also be specified on command line as key.subkey..=value|value,value

    {
        "defaults" : { ... defaults for all other sections ... }

        "client" : { ... configuration for connecting to meeseeks ... }

        "watch" : {
                    <name>: { 
                                "path" : <path to watch>
                                "glob" : <pattern> | [ <pattern>, ... ]
                                         Watches the files matching the pattern. 
                                         A list of patterns can be specified to watch multiple lists of files
                                "reverse" : <bool> (files are ASCII sorted Z->A, 9->0 to handle datestamps newest to oldest. 
                                                    If true, reverse sort the files (oldest to newest))
                                "split" : <character> (optional character to split filenames on to generate match parts)
                                "match" : <int> match partial filenames across lists to create filesets
                                            a fileset is complete when the first <int> parts of a filename in each list matches.
                                            files will not be processed until a complete fileset exists. 
                                            For example,
                                             glob: [*.foo,*.bar,*.baz]
                                             split: .
                                             match: 2
                                             the set will be complete if we have 20200101.00.foo, 20200101.00.bar, 20200101.00.baz
                                          default is 0 (no filesets matching)
                                "updated" : <bool> if set, files will be reprocessed if modtime changes. Default false.
                                "retry" : <bool> if false, files with failed jobs will be considered processed. Default True
                                "min_age" : <int> if set, file must be at least <int> seconds old to be considered
                                "max_age" : <int> if set, file must be newer than <int> seconds to be considered
                                "max_index": <int> if set, maximum index down the list we will submit jobs for.
                                "refresh" : <int> interval in seconds running jobs are checked and file status updated, default 10
                                "rescan"  : <int> interval in seconds files in path are rescanned, default 60

                                "jobs" : [
                                    { jobspec } | [ {jobspec}, ... ] ,
                                        jobspec(s) to submit on first (usually newest) file/fileset
                                    ,   
                                        {} | [{},...]
                                        jobspec(s) to submit on next file/fileset...
                                    , ..... ,
                                        {} | [{},...]
                                        last jobspec(s) in list will be submitted on all other unprocessed files
                                ]

                                if only one job is defined, this job will be run on all files
                                if multiple jobs are defined, files will have the jobs run on them in sequence as more files appear
                                for example, when a new file[0] appears:
                                    jobs[0] will run on new file[0]
                                    previously processed file[0] will now be file[1], so jobs[1] will run on it
                                    file[1] will now be file[2] but if jobs[2] does not exist nothing happens.

                                if a job is a list of jobspecs:
                                    the first jobspec will be submitted for files from the first list,    
                                    the second jobspec for the second list, and so on.
                                    files with no associated jobspec or an empty/null jobspec will be immediately marked as processed
        
                                in jobspecs, the following formats are available for strings:
                                    %(name)s        name of this watch
                                    %(filename)s    filename
                                    %(file)s        full path to file including filename
                                    %(<n>)s         part <n> of the filename (if split set)
                                    %(<k>)s         key <k> in the watch config

                                When a file job is exits, the job result JSON is written to a hidden file in <path> named:
                                ._<name>_<n>_<file>.<state>
                                <name> is the name of the watch
                                <n> is the index of the job in the jobspec list (0=jobspec for first, etc.. )
                                <file> is the filename
                                <state> is the finished job state, typically "done" if successful. 
                                        If a .done file exists the file is considered processed

                                If updated=True, last file mtime is tracked in ._<name>_<n>_<file>.mtime

                                hidden ._ files will be deleted when associated files are deleted, unless cleanup=0
                    } 
        }
    }
""")

def apply_config(**cfg):
    global CFG,CLIENT,WATCH
    logging.info('reloading config')
    CFG.update(cfg)
    #load config defaults
    defaults=CFG.get('defaults',{})

    #init client
    if not CLIENT: 
        ccfg={'refresh':10}
        ccfg.update(defaults)
        ccfg.update(cfg.get('client',{}))
        CLIENT=Client(**ccfg)

    #stop/start watch threads
    watch=cfg.get('watch',{})
    for w in WATCH.copy(): 
        if w not in watch: stop_watch(w)
    for w in watch.keys():
        if w not in WATCH:
            wcfg=defaults.copy()
            wcfg.update(watch[w])
            logging.info('adding watch %s'%w)
            WATCH[w]=Watch(w,client=CLIENT,**wcfg)

def stop_watch(w):
    global WATCH
    wt=WATCH[w]
    wt.shutdown.set()
    logging.info('stopping %s'%wt.name)
    wt.join()
    del WATCH[w]

def signal_stop(*args):
    global SHUTDOWN
    logging.debug(args)
    SHUTDOWN.set()

def signal_reload(*args):
    logging.debug(args)
    apply_config(**config())

def config():
    cfgargs,args=cmdline_parser(sys.argv[1:])
    cfg=read_cfg_files(args)
    cfg.update(cfgargs)
    logging.basicConfig(**cfg.get('logging',{'level':logging.INFO}))
    return cfg

signal.signal(signal.SIGINT,signal_stop)
signal.signal(signal.SIGTERM,signal_stop)
signal.signal(signal.SIGHUP,signal_reload)

#do we have args?
if len(sys.argv) < 2: sys.exit(usage())

#get config
apply_config(**config())

#slow ._* cleanup
while not SHUTDOWN.is_set(): 
    if CFG.get('cleanup'):
        paths=[w.path for w in WATCH.values()]
        for path in paths:
            if SHUTDOWN.is_set(): break
            logging.debug('cleaning up %s'%path)
            hidden_files=glob.glob(os.path.join(path,'._*'))
            for hf in hidden_files:
                if SHUTDOWN.is_set(): break
                #from ._name_index_file.state get file.state, strip off state
                file='.'.join( os.path.split(hf)[1].split('_',4)[3].split('.')[:-1] )
                try:
                    if not os.path.exists(os.path.join(path,file)):
                        logging.debug('%s %s does not exist, removed %s'%(path,file,hf))
                        os.unlink(hf)
                except Exception as e: logging.debug(e)
            time.sleep(1)
        c=0
        while not SHUTDOWN.is_set():
            c+=1
            time.sleep(1)
            if c > CFG['cleanup']: break
    else: time.sleep(1)

#shut down all watches
for w in list(WATCH.keys()): stop_watch(w)

#close client gracefully to sync final state
if CLIENT: CLIENT.close()