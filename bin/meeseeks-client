#!/usr/bin/env python3

import os
import sys
import logging
import json
import time

from meeseeks import Client
from meeseeks.util import cmdline_parser, read_cfg_files

#handle cmd from symlink to meeseeks-client
EXEC_NAME=os.path.basename(sys.argv[0])
if EXEC_NAME.startswith('q'):cmd=EXEC_NAME[1:]
else: cmd=None

def usage():

    print("""
    q{stat|sub|del|mod|get|conf} [args]
    %s [options] <command> [args...]

    commands are:
        sub <pool[@node]> <executable> [args....]
            options for submit:
                filter= (list of nodes to route job through)
                stdin= stdout= stderr= (redirect job in/out/err to files named)
                restart= (1=restart when done)
                retries= (times to retry if job fails)
                runtime= (max runtime of job)
                id= (job id to submit changes to existing job)
                state= (change existing job state, 'new' will restart finished job)
                hold= (1=queue but do not start job)

        get [jobids | filter] (get all or specified job info as JSON)
            jobids are any number of job id arguments
            filter is any job attributes such as node= pool= state=

        ls [filter] (list job ids)

        set|mod <jobids | filter :> key=value ... (set key=value in jobs matching jobids or filter)
            if a filter is provided, : is used to delimit filter key=value from set key=value

        del|kill <jobids | filter> (kill job)

        reset <jobids | filter> (restart job)

        stat|show [filter] {nodes pools jobs active tree} 
            (prints a foat or tree of cluster status, 
             specify which elements to show, defaults to flat/all)

        nodes (prints full node status JSON)

        pools (prints full pool status JSON)

        conf [key=value] [node]
            get/sends config to directly connected or specified node

    generic options are: 
        address= (default localhost)
        port= (defult 13700)
        refresh= (interval to continuously refresh status until no jobs left)

"""%EXEC_NAME)

def pretty_print(d): print (json.dumps(d, sort_keys=True, indent=4))

def show(client,args,**kwargs):     
    args=[arg[0] for arg in args]
    status=client.status() 
    nodes=status.get('nodes',{})
    pools=status.get('pools',{})
    kwpool=kwargs.get('pool')
    if kwpool: del kwargs['pool']
    kwnode=kwargs.get('node')
    if kwnode: del kwargs['node']
    if 'n' not in args and 'p' not in args and 'j' not in args:
        args.extend(['n','p','j'])
    #put pool and job status on nodes
    for n,node in nodes.items():
        if kwnode and n != kwnode: continue
        for p,pnodes in pools.items():
            if kwpool and p != kwpool: continue
            if n in pnodes: node.setdefault('free',{})[p]=pnodes[n]
        jobs=client.query(node=n,**kwargs)
        for jid,job in jobs.items(): node.setdefault('jobs',{}).setdefault(job['pool'],{})[jid]=job
    #now we put nodes under other nodes
    def _tree(s):
        for n in list(s.keys()):
            if n in s: #already moved?
                node=s[n]
                if not node.get('online'): continue
                for dn in node.get('routing',[]):
                    if dn in s and dn != n: #we can move this node
                        s[n].setdefault('next',{})[dn]=s[dn]
                        del s[dn]
                if s[n].get('next'): _tree(s[n]['next']) #walk downstream
    if 't' in args: _tree(nodes)
    #now we print the tree
    def _print(s,i=0):
        for n,node in sorted(s.items()):
            if 'n' in args:
                l='\t'*i + n+'\t'
                if node.get('online'): l+='online'
                else: l+='offline'
                l+='\t%s'%node.get('loadavg')
                if n: print(l)
            pn=set(node.get('free',{}).keys())|set(node.get('jobs',{}).keys())
            for p in sorted(pn):
                if 'n' in args: l='\t'*i + '\t'
                else: l=''
                l+='pool %s@%s'%(p,n)
                jobs=node.get('jobs',{}).get(p,{})
                jc=len(jobs)
                l+='\t%s jobs'%jc
                free=node.get('free',{}).get(p)
                if type(free) is int: l+='\t%s slots free'%free
                if 'p' in args and ('a' not in args or jc): print (l)
                l=''
                if 'n' in args: l+='\t'*i+'\t'
                if 'p' in args: l+='\t'
                if 'j' in args:
                    for (jid,job) in sorted(jobs.items(),key=lambda j: (j[1].get('state')) ):
                        flags=''
                        if job.get('active'): 
                            flags+='A'
                            if 'start_ts' in job: rc=str(int(job['ts']-job['start_ts']))
                            else: rc=None
                        else: rc=job.get('rc')
                        if job.get('error'): flags+='E'
                        if job.get('hold'): flags+='H'
                        if job.get('restart'): flags+='R'
                        print(l+'%s\t%s\t%s\t%s\t%s'%(
                            jid,flags,job['state'],rc,' '.join(job.get('args',[]))) )
            if 'next' in node:
                _print(node['next'],i+1)
    _print(nodes,0)

# cmd=first non cfg argument
# args=remaining arguments 
try:
    cfg,env=cmdline_parser(os.getenv('MEESEEKS_CONF','').split())
    args=sys.argv[1:]
    if not cmd:
        c,args=cmdline_parser(args)
        cfg.update(c)
        cmd,args=args[0],args[1:] #end of options
    if 'file' in cfg: cfg.update(read_cfg_files(cfg['file']))
    logging.basicConfig(level=cfg.get('log',logging.INFO))
    logging.debug('%s %s %s'%(cfg,cmd,args))
    client=Client(**cfg) #create client and connect
    jids,query=None,{}
    #if we will be refreshing, refresh now until state populated
    if cfg.get('refresh'):
        while not client.get_node_status(): time.sleep(1)

    if cmd.startswith('sub'):
        #get pool, args and optional node selection
        pool=jobargs=node=None
        if args: 
            pool=args[0]
            if len(args)>1: jobargs=args[1:]
        if pool and '@' in pool: pool,node=pool.split('@',1)
        #if we are refreshing on the job, submit locally and let it dispatch
        if cfg.get('refresh'): jids=[client.submit_job(pool=pool,args=jobargs,node=node,**cfg)]
        #submit directly and return jid
        else: print(client.submit(pool=pool,args=jobargs,node=node,**cfg))

    else: #if not submit, parse as key=value args and job ids
        kwargs,args=cmdline_parser(args)
        if cmd == 'get': #get jobs by ids or filter on key=value
            if cfg.get('refresh'):
                if args: jids=args
                else: jids=True #get all
            elif args: pretty_print(client.query(ids=args))
            else: pretty_print(client.query(**kwargs))

        elif cmd == 'set' or cmd == 'mod': #set k=v in matching jobs
            #kwargs will be set if we have initial k=v args like 'pool=... : key=...' 
            if kwargs: jids=client.ls(**kwargs) #we were provided a filter
            else: jids=[arg for arg in args if '=' not in arg] #no filter, we have 'jobid jobid key=...'
            kwargs,args=cmdline_parser([arg for arg in args if '=' in arg]) #parse following k=v args
            for jid in jids: client.submit(id=jid,**kwargs) #submit changes 
            if jids: pretty_print(client.query(ids=jids)) #print changes 

        elif cmd == 'ls': pretty_print(client.ls(**kwargs))

        elif cmd == 'kill' or cmd == 'del': pretty_print(client.kill(args,**kwargs))

        elif cmd == 'reset': #reset jobs
            #to reset active jobs we have to kill them and wait for active=False
            jids=list(jid for (jid,job) in client.kill(args,**kwargs).items() if job)
            while jids: #while we have remaining jobs
                #restart jobs that are now inactive
                for jid in list(jid for (jid,job) in client.query(ids=jids).items() if not job.get('active')):
                    client.submit(id=jid,state='new') #submit to existing and change state to new, this resets job
                    pretty_print(client.query(jid))
                    jids.remove(jid) #remove from list
                if jids:
                    time.sleep(cfg.get('refresh',1)) #if some jobs are still active, wait and try again

        elif cmd == 'nodes': 
            if cfg.get('refresh'): 
                while True:
                    pretty_print(client.get_node_status())
                    time.sleep(cfg.get('refresh'))
            else: pretty_print(client.status()['nodes'])

        elif cmd == 'pools': 
            if cfg.get('refresh'): 
                while True:
                    pretty_print(client.get_pool_status())
                    time.sleep(cfg.get('refresh'))
            else: pretty_print(client.status()['pools'])

        elif cmd.startswith('conf'): 
            if 'file' in kwargs: kwargs.update(read_cfg_files(kwargs['file']))
            if args:
                for node in args: 
                    print(client.submit(pool='__config',node=node,args=kwargs))
            else: pretty_print(client.request(dict(config=kwargs))) #peer config push

        elif cmd == 'show' or cmd.startswith('stat'): show(client,args,**kwargs) #pretty print full cluster

        else: sys.exit(usage())
    
    #refreshing on jobs
    if jids and cfg.get('refresh'):
        seq={} #sequence numbers for printing updates
        while True: 
            if jids is True: jobs=client.get(**kwargs)
            else: jobs=dict((jid,client.get_job(jid)) for jid in jids)
            if not any(jobs.values()): break #exit when no jobs left
            #print updates (when job seq != saved seq)
            updates=dict((jid,job) for (jid,job) in jobs.items() if job['seq'] != seq.get(jid))
            if updates: pretty_print(updates)
            seq=dict((jid,job['seq']) for (jid,job) in jobs.items())
            time.sleep(cfg.get('refresh'))
except Exception as e: print (e,file=sys.stderr)

#if we were polling state, stop gracefully
if cfg.get('refresh'): client.close()